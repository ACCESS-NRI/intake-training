{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "751ea4c4-a75f-48ab-9756-7ee6e06b290f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function use_datastore in module access_nri_intake.experiment.main:\n",
      "\n",
      "use_datastore(experiment_dir: pathlib.Path | str, builder: ecgtools.builder.Builder | None = None, catalog_dir: pathlib.Path | str | None = None, builder_kwargs: dict | None = None, open_ds: bool = True, datastore_name: str = 'experiment_datastore', description: str | None = None) -> intake_esm.core.esm_datastore | None\n",
      "    Specify a builder and an experiment directory in order to build and/or open\n",
      "    an esm-datastore in place for that experiment. Valid and up to date datastores\n",
      "    will not be overwritten.\n",
      "    \n",
      "    Further configuration can be done by passing additional keyword arguments\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    builder : Builder\n",
      "        The builder object that will be used to build the datastore.\n",
      "    experiment_dir : Path | str\n",
      "        The directory containing the experiment. If a string is passed, it will be\n",
      "        converted to a Path object.\n",
      "    catalog_dir : Path | str, optional\n",
      "        The directory containing/to write the catalog to, if it differs from the\n",
      "        experiment directory. If None, the catalog will be written to the experiment\n",
      "        directory. If a string is passed, it will be converted to a Path object.\n",
      "    open_ds : bool\n",
      "        Whether to open the datastore after building it. Typically set to false\n",
      "        when called from a console script.\n",
      "    builder_kwargs : dict, optional\n",
      "        Any additional keyword arguments to pass to the builder if needed - for\n",
      "        example, AccessEsm15Builder additionally takes an `ensemble` argument\n",
      "    datastore_name : str, optional\n",
      "        The name of the datastore to be written. Defaults to 'experiment_datastore'.\n",
      "        Datastores are written as `catalog_dir / datastore_name.json`.\n",
      "    description : str, optional\n",
      "        A description of the datastore. If None, a default description will be used.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    esm_datastore | None\n",
      "        The datastore object, if it was requested to be opened. Otherwise, None.\n",
      "    \n",
      "    Raises\n",
      "    ------\n",
      "    TBC\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from access_nri_intake.experiment import use_datastore\n",
    "\n",
    "help(use_datastore)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda19560-b3a2-42cb-a4df-c16ccc1eda4e",
   "metadata": {},
   "source": [
    "# Exercise 1: Building a datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b942ba-a992-40ad-ab93-ab7f079742a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We will make a directory the intake-training repo to save our datastore in.\n",
    "# If you cloned the repo from somewhere other than your home directory,\n",
    "# you may need to alter the `CATALOG_DIR` variable\n",
    "from access_nri_intake.source import builders\n",
    "import warnings\n",
    "import os\n",
    "from xarray import SerializationWarning\n",
    "warnings.filterwarnings(action='once',category=UserWarning)\n",
    "warnings.filterwarnings(action='once',category=SerializationWarning)\n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
    "\n",
    "!mkdir ~/intake-training/2025_02/cat_dir\n",
    "!cd \n",
    "# NB: There is currently a bug in use_datastore with homedir expansion (ie. ~/), so please use either relative paths \n",
    "# or absolute paths (ie. /home/user/). We run cd above so we don't need to start paths with ~/\n",
    "\n",
    "CATALOG_DIR = \"intake-training/2025_02/cat_dir\"\n",
    "EXPERIMENT_DIR = '/g/data/ik11/outputs/access-om2/1deg_iamip2_CMCC-ESM2ssp126'\n",
    "BUILDER = builders.AccessOm2Builder\n",
    "\n",
    "expt_datastore = use_datastore(\n",
    "    experiment_dir = EXPERIMENT_DIR,\n",
    "    catalog_dir = CATALOG_DIR,\n",
    "    builder=BUILDER\n",
    ")\n",
    "expt_datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae9db11-6a84-40cb-b639-2b5e0a3b2640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets rerun use_datastore to verify that the datastore doesn't rebuild when the datastore hasn't changed\n",
    "expt_datastore = use_datastore(\n",
    "    experiment_dir = EXPERIMENT_DIR,\n",
    "    catalog_dir = CATALOG_DIR,\n",
    "    builder=BUILDER\n",
    "\n",
    ")\n",
    "expt_datastore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bd8328-2d30-4db4-98e9-4419e36cd51d",
   "metadata": {},
   "source": [
    "___\n",
    "# Exercise 2: Searching for a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd325535-deef-480d-8889-ed27c4a06a63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/proxy/8787/status'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In order to open our datasets, we'll need a dask cluster: so lets start that.\n",
    "\n",
    "from distributed import Client\n",
    "\n",
    "client = Client(threads_per_worker=1)\n",
    "client.dashboard_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da37157-99cd-4843-8a37-676f9f83aca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datastore.df.head() can often be a good way to get a feel for the datastore\n",
    "expt_datastore.df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657c05b8-7351-40af-84ed-1ffb18b05910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets pick a frequency - first, we can check all the unique frequencies:\n",
    "expt_datastore.df.frequency.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aef3cac-d520-4bc2-8d58-b447218dd9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how many datasets are at monthly frequency\n",
    "expt_datastore.search(frequency=\"1mon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35f754f-1dc7-44e9-a7d4-2e7cdadf4163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have two datasets - and it looks like we can split them on file_id:\n",
    "expt_datastore.search(frequency=\"1mon\").df.file_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bb953c-6805-4830-9104-21a0bf02252b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets look at physical variables:\n",
    "expt_datastore.search(frequency=\"1mon\",file_id=\"ocean_scalar_1_monthly_ym_XXXX_XX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa0f358-f0f8-478e-a3d5-539e9c039933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a single dataset - so we can load the whole thing with .to_dask()\n",
    "expt_datastore.search(frequency=\"1mon\",file_id=\"ocean_scalar_1_monthly_ym_XXXX_XX\").to_dask()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8bb2ad-4f70-43e6-92c1-5346043adf27",
   "metadata": {},
   "source": [
    "## Now, try exploring the datastore yourself:\n",
    "- Can you load a single year from a single variable into a dataset?\n",
    "- What other ways can you extract a subset of data from the datastore?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10930d3a-37a3-45a5-9443-fafe2bb75a43",
   "metadata": {},
   "source": [
    "___\n",
    "# Exercise 3: Loading multiple datasets at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce88c42-4831-4b77-a184-6487a5ea3823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What about if we wanted one physical, and one biogeochemical variable? \n",
    "# Say, `temp_global_ave` and `total_co2_flux`\n",
    "expt_datastore.search(frequency=\"1mon\",variable=['temp_global_ave', 'total_co2_flux'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a871bc5f-846c-46b9-87be-b05bae96b711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# They're in different datasets, so we can't just use .to_dask(). Instead,\n",
    "# we can use .to_dataset_dict() - and we'll save them into the dict `d`\n",
    "d = expt_datastore.search(frequency=\"1mon\",variable=['temp_global_ave', 'total_co2_flux']).to_dataset_dict()\n",
    "\n",
    "for key in d.keys():\n",
    "    print(key)\n",
    "\n",
    "# When we search for variables, intake is smart enough to only load the ones we searched for.\n",
    "d['ocean_scalar_1_monthly_ym_XXXX_XX.1mon']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e97dc6a-6a3b-41dd-a8e6-9c6d90210763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When we search for variables, intake is smart enough to only load the ones we searched for.\n",
    "d['oceanbgc_scalar_1_monthly_ym_XXXX_XX.1mon']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d272012-211d-476e-ac1a-8b5f576cb64d",
   "metadata": {},
   "source": [
    "## Now, try exploring the datastore yourself:\n",
    "- Can you load the same variable at multiple frequencies into a dataset dict?\n",
    "- What other ways can you extract a subset of data from the datastore?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8f6b48-8f30-4b1a-b7ac-2ce8f74c10db",
   "metadata": {},
   "source": [
    "___\n",
    "# Exercise 4: Searching on standard names\n",
    "\n",
    "We'll use the regular catalog here - the one we just built doesn't have any useful standard named variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc694884-1ec3-43cb-9b5f-0c0d4e6227a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import intake\n",
    "esm_ds = intake.cat.access_nri['1deg_jra55_ryf9091_gadi']\n",
    "esm_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0d1c85-8865-45c1-a4be-bba37e063740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets look for sea surface height\n",
    "esm_ds.search(variable_standard_name=\"sea_surface_height_above_geoid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99325d82-6ff5-4d4c-8fe6-0febb7c8eb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We get one dataset - so lets open it\n",
    "esm_ds.search(variable_standard_name=\"sea_surface_height_above_geoid\").to_dask()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e4c463-c30b-4f99-b20c-acca709f57a3",
   "metadata": {},
   "source": [
    "# Exercise 5 (Bonus): Using your own datastore for a recipe\n",
    "\n",
    "The dataset contained at `/g/data/cj50/access-om2/raw-output/access-om2-01/01deg_jra55_ryf_Control/` contains a number of sea surface height data. \n",
    "\n",
    "Lets build a catalog for it, and then use it for the recipe at https://github.com/COSIMA/cosima-recipes/blob/main/Recipes/Compare_SSH_model_obs.ipynb\n",
    "\n",
    "Note: This dataset can be found in the standard catalog - you can compare it against the one you build."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b67725-1845-4157-bb07-05c4cddafcd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cd \n",
    "!mkdir intake-training/2025_02/cosima_recipe_datastore\n",
    "\n",
    "CATALOG_DIR = \"intake-training/2025_02/cat_dir\" # What does this need to change to?\n",
    "EXPERIMENT_DIR = '/g/data/ik11/outputs/access-om2/1deg_iamip2_CMCC-ESM2ssp126' # What does this need to change to?\n",
    "BUILDER = builders.AccessOm2Builder # Does this need to change?\n",
    "\n",
    "expt_datastore = use_datastore(\n",
    "    experiment_dir = EXPERIMENT_DIR,\n",
    "    catalog_dir = CATALOG_DIR,\n",
    "    builder=BUILDER\n",
    ")\n",
    "expt_datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf42746-db71-46cb-89af-4b9ee415069b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
